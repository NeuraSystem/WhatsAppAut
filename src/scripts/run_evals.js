// src/scripts/run_evals.js

require("dotenv").config();
const fs = require("fs");
const path = require("path");
const { SimpleAgentExecutor } = require("../services/simpleAgentExecutor");
const { initializeDatabase } = require("../database/pg");
const logger = require("../utils/logger");

const EVAL_DATASET_PATH = path.join(__dirname, "../../eval_dataset.json");

async function runEvals() {
  logger.info("ðŸš€ Iniciando ejecuciÃ³n de evaluaciones...");

  try {
    // Inicializar la base de datos (necesario para algunas herramientas)
    await initializeDatabase();
    logger.info("ðŸ—„ï¸ Base de datos inicializada para evaluaciones.");

    // Inicializar el AgentExecutor
    const agentExecutor = new SimpleAgentExecutor();
    logger.info("ðŸ§  SimpleAgentExecutor inicializado para evaluaciones.");

    // Cargar el dataset de evaluaciones
    const evalData = JSON.parse(fs.readFileSync(EVAL_DATASET_PATH, "utf-8"));
    const evaluations = evalData.evaluaciones;
    logger.info(`ðŸ“‹ Cargadas ${evaluations.length} evaluaciones del dataset.`);

    let passedCount = 0;
    let failedCount = 0;

    for (const evalCase of evaluations) {
      logger.info(`
--- Ejecutando evaluaciÃ³n ${evalCase.id} (${evalCase.categoria}) ---`);
      logger.info(`Pregunta: "${evalCase.pregunta}"`);

      let agentResponse;
      try {
        const result = await agentExecutor.execute(evalCase.pregunta);
        agentResponse =
          typeof result === "string"
            ? result
            : result.response || JSON.stringify(result);
        logger.info(`Respuesta del Agente: "${agentResponse}"`);
      } catch (error) {
        logger.error(`Error al ejecutar el agente para ${evalCase.id}:`, error);
        agentResponse = `ERROR: ${error.message}`;
      }

      const isPassed = evalCase.respuesta_ideal_contiene.every((keyword) =>
        String(agentResponse).toLowerCase().includes(keyword.toLowerCase()),
      );

      if (isPassed) {
        passedCount++;
        logger.info(`âœ… EVALUACIÃ“N ${evalCase.id}: PASÃ“`);
      } else {
        failedCount++;
        logger.error(`âŒ EVALUACIÃ“N ${evalCase.id}: FALLÃ“`);
        logger.error(
          `   Esperaba contener: ${evalCase.respuesta_ideal_contiene.join(", ")}`,
        );
        logger.error(`   Respuesta obtenida: ${agentResponse}`);
      }
    }

    logger.info("\n--- RESUMEN DE EVALUACIONES ---");
    logger.info(`Total de evaluaciones: ${evaluations.length}`);
    logger.info(`Evaluaciones PASADAS: ${passedCount}`);
    logger.info(`Evaluaciones FALLIDAS: ${failedCount}`);

    if (failedCount > 0) {
      logger.error(
        "ðŸ’¥ ALGUNAS EVALUACIONES FALLARON. Revisa los logs para mÃ¡s detalles.",
      );
      process.exit(1);
    } else {
      logger.info("ðŸŽ‰ TODAS LAS EVALUACIONES PASARON EXITOSAMENTE.");
      process.exit(0);
    }
  } catch (error) {
    logger.error("Error crÃ­tico durante la ejecuciÃ³n de evaluaciones:", error);
    process.exit(1);
  }
}

if (require.main === module) {
  runEvals();
}
